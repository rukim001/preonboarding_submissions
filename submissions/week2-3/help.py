# -*- coding: utf-8 -*-
"""help.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/128yGgkxt7Mx1sSXcxxeFUVa3H70Ex7vZ
"""

import os
import sys
import pandas as pd
import numpy as np 
import torch
import random

from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, random_split

!pip install transformers

from transformers import BertTokenizer, BertModel

tokenizer_bert = BertTokenizer.from_pretrained("klue/bert-base")

class CustomDataset(Dataset):
  """
  - input_data: list of string
  - target_data: list of int
  """

  def __init__(self, input_data:list, target_data:list) -> list:
      self.X = list(input_data)
      self.Y = list(target_data)

  def __len__(self):
      return len(self.X)

  def __getitem__(self, index):
      return self.X[index], self.Y[index]

class CustomClassifier(nn.Module):

  def __init__(self, hidden_size: int, n_label: int):
    super(CustomClassifier, self).__init__()

    self.bert = BertModel.from_pretrained("klue/bert-base")

    dropout_rate = 0.1
    linear_layer_hidden_size = 32

    self.classifier = nn.Sequential(
        nn.Linear(hidden_size, linear_layer_hidden_size),
        nn.ReLU(),
        nn.Dropout(p=dropout_rate),
        nn.Linear(linear_layer_hidden_size, n_label)
    ) # torch.nn에서 제공되는 Sequential, Linear, ReLU, Dropout 함수 활용
  
  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):
    outputs = self.bert(
        input_ids,
        attention_mask=attention_mask,
        token_type_ids=token_type_ids,
    )

    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱
    cls_token_last_hidden_states = outputs['pooler_output'] # 마지막 layer의 첫 번째 토큰 ("[CLS]") 벡터를 가져오기, shape = (1, hidden_size)

    logits = self.classifier(cls_token_last_hidden_states)

    return logits

def set_device():
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def custom_collate_fn(batch):

    global tokenizer_bert

    input_list, target_list = [], []

    for (_text, _label) in batch:
        target_list.append(_label)
        input_list.append(_text)

        tensorized_input = tokenizer_bert(
                            input_list,
                            add_special_tokens=True,
                            return_tensors='pt',
                            padding='longest',
                            truncation=True
                            )
  
    tensorized_label = torch.tensor(target_list)
  
    return tensorized_input, tensorized_label